{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing record file, delete if you want to re-fetch\n"
     ]
    }
   ],
   "source": [
    "#Import functions and load data\n",
    "import os\n",
    "os.chdir(\"../src\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from dataloader import qm9_parse, qm9_fetch\n",
    "import dmol\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "qm9_records = qm9_fetch()\n",
    "data = qm9_parse(qm9_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data = data.shuffle(7000, reshuffle_each_iteration=False)\n",
    "test_set = shuffled_data.take(1000)\n",
    "valid_set = shuffled_data.skip(1000).take(1000)\n",
    "train_set = shuffled_data.skip(2000).take(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def convert_record(d, atom_types=100, embedding_dim=128):\n",
    "    # break up record\n",
    "    (e, x), y = d\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    e = torch.tensor(e.numpy())\n",
    "    x = torch.tensor(x.numpy())\n",
    "    r = x[:, :3]\n",
    "\n",
    "    # Assuming atom indices start from 1\n",
    "    e = e - 1\n",
    "    e = torch.clamp(e, 0, atom_types - 1)  # Ensure indices are within valid range\n",
    "\n",
    "    # Embedding\n",
    "    embedding_layer = nn.Embedding(num_embeddings=atom_types, embedding_dim=embedding_dim)\n",
    "    s = embedding_layer(e)\n",
    "\n",
    "    return (s, r), y.numpy()[13]  # Select attribute at index 13\n",
    "\n",
    "\n",
    "#\n",
    "def x2e(x, cutoff_distance=5.0):\n",
    "    \"\"\"convert xyz coordinates to pairwise distance with a cutoff distance\"\"\"\n",
    "   # Calculate pairwise distances\n",
    "   # this calculates the norm\n",
    "    #r0 = (x- x[:, None, :]) #TODO: RIJ \n",
    "    r2 = torch.sqrt(((x - x[:, None, :])**2).sum(dim=-1))\n",
    "\n",
    "    # Create a mask for distances less than cutoff_distance\n",
    "    mask = (r2>0) & (r2 <= cutoff_distance)\n",
    "\n",
    "    # Use the mask to set values in the tensor\n",
    "    r_ij = torch.where(mask, r2, torch.zeros_like(r2))\n",
    "\n",
    "    # Generate edge index matrix\n",
    "    #edge_index = torch.nonzero(mask, as_tuple=False)\n",
    "\n",
    "    #edge_mask = (r2 > 0) & (r2 < cutoff_distance)\n",
    "    edge_indices = mask.nonzero(as_tuple=True)\n",
    "    edge_index = torch.stack(edge_indices)\n",
    "    #edge_index = edge_index.resize_(2,len(mask))\n",
    "\n",
    "    return r_ij, edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in test_set:\n",
    "    (s, r_ij), y_raw = convert_record(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize y values first and transform after prediction\n",
    "ys = [convert_record(d)[1] for d in train_set]\n",
    "train_ym = np.mean(ys)\n",
    "train_ys = np.std(ys)\n",
    "def transform_label(y):\n",
    "    return (y - train_ym) / train_ys\n",
    "def transform_prediction(y):\n",
    "    return y * train_ys + train_ym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineCutoff(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, cutoff=5.0):\n",
    "        super(CosineCutoff, self).__init__()\n",
    "        #self.register_buffer(\"cutoff\", torch.FloatTensor([cutoff]))\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def forward(self, distances):\n",
    "        \"\"\"Compute cutoff.\n",
    "\n",
    "        Args:\n",
    "            distances (torch.Tensor): values of interatomic distances.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: values of cutoff function.\n",
    "\n",
    "        \"\"\"\n",
    "        # Compute values of cutoff function\n",
    "        cutoffs = 0.5 * (torch.cos(distances * np.pi / self.cutoff) + 1.0)\n",
    "        # Remove contributions beyond the cutoff radius\n",
    "        cutoffs *= (distances < self.cutoff).float()\n",
    "        return cutoffs\n",
    "\n",
    "class BesselBasis(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Sine for radial basis expansion with coulomb decay. (0th order Bessel from DimeNet)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cutoff=5.0, n_rbf=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cutoff: radial cutoff\n",
    "            n_rbf: number of basis functions.\n",
    "        \"\"\"\n",
    "        super(BesselBasis, self).__init__()\n",
    "        # compute offset and width of Gaussian functions\n",
    "        freqs = torch.arange(1, n_rbf + 1) * math.pi / cutoff\n",
    "        self.register_buffer(\"freqs\", freqs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = torch.norm(inputs, p=2, dim=1)\n",
    "        a = self.freqs\n",
    "        ax = torch.outer(inputs,a)\n",
    "        sinax = torch.sin(ax)\n",
    "\n",
    "        norm = torch.where(inputs == 0, torch.tensor(1.0, device=inputs.device), inputs)\n",
    "        y = sinax / norm[:,None]\n",
    "\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing, radius_graph\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "import ase\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "from torch.nn import Embedding, Sequential, Linear, ModuleList, Module\n",
    "import numpy as np\n",
    "from torch import linalg as LA\n",
    "import math\n",
    "\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassPaiNN(MessagePassing):\n",
    "    def __init__(self, num_feat, out_channels, num_nodes, cut_off=5.0, n_rbf=20):\n",
    "        super(MessagePassPaiNN, self).__init__(aggr='add') \n",
    "        \n",
    "        self.lin1 = Linear(num_feat, out_channels) \n",
    "        self.lin2 = Linear(out_channels, 3*out_channels) \n",
    "        self.lin_rbf = Linear(n_rbf, 3*out_channels) \n",
    "        self.silu = Func.silu\n",
    "        \n",
    "        #self.prepare = Prepare_Message_Vector(num_nodes)\n",
    "        self.RBF = BesselBasis(cut_off, n_rbf)\n",
    "        self.f_cut = CosineCutoff(cut_off)\n",
    "        self.num_nodes = num_nodes\n",
    "    \n",
    "    def forward(self, s,v, edge_index, edge_attr):\n",
    "        \n",
    "        s = s.flatten(-1)\n",
    "        v = v.flatten(-2)\n",
    "        \n",
    "        flat_shape_v = v.shape[-1]\n",
    "        flat_shape_s = s.shape[-1]\n",
    "    \n",
    "        x =torch.cat([s, v], dim = -1)\n",
    "        \n",
    "        \n",
    "        x = self.propagate(edge_index, x=x, edge_attr=edge_attr\n",
    "                            ,flat_shape_s=flat_shape_s, flat_shape_v=flat_shape_v)\n",
    "            \n",
    "        return x    \n",
    "    \n",
    "    def message(self, x_j, edge_attr, flat_shape_s, flat_shape_v):\n",
    "        \n",
    "        \n",
    "        # Split Input into s_j and v_j\n",
    "        s_j, v_j = torch.split(x_j, [flat_shape_s, flat_shape_v], dim=-1)\n",
    "        \n",
    "        # r_ij channel\n",
    "        rbf = self.RBF(edge_attr)\n",
    "        ch1 = self.lin_rbf(rbf)\n",
    "        cut = self.f_cut(edge_attr.norm(dim=-1))\n",
    "        W = torch.einsum('ij,i->ij',ch1, cut) # ch1 * f_cut\n",
    "        \n",
    "        # s_j channel\n",
    "        phi = self.lin1(s_j)\n",
    "        phi = self.silu(phi)\n",
    "        phi = self.lin2(phi)\n",
    "        \n",
    "        # Split \n",
    "        left, dsm, right = torch.tensor_split(phi*W,3,dim=-1)\n",
    "        \n",
    "        # v_j channel\n",
    "        normalized = Func.normalize(edge_attr, p=2, dim=1)\n",
    "        \n",
    "        v_j = v_j.reshape(-1, int(flat_shape_v/3), 3)\n",
    "        hadamard_right = torch.einsum('ij,ik->ijk',right, normalized)\n",
    "        hadamard_left = torch.einsum('ijk,ij->ijk',v_j,left)\n",
    "        dvm = hadamard_left + hadamard_right \n",
    "        \n",
    "        # Prepare vector for update\n",
    "        x_j = torch.cat((dsm,dvm.flatten(-2)), dim=-1)\n",
    "       \n",
    "        return x_j\n",
    "    \n",
    "    def update(self, out_aggr,flat_shape_s, flat_shape_v):\n",
    "        \n",
    "        s_j, v_j = torch.split(out_aggr, [flat_shape_s, flat_shape_v], dim=-1)\n",
    "        \n",
    "        return s_j, v_j.reshape(-1, int(flat_shape_v/3), 3)\n",
    "class MessagePassPaiNN_NE(MessagePassing):\n",
    "    def __init__(self, num_feat, out_channels, num_nodes, cut_off=5.0, n_rbf=20):\n",
    "        super(MessagePassPaiNN_NE, self).__init__(aggr=\"add\")\n",
    "\n",
    "        self.lin1 = Linear(num_feat, out_channels)\n",
    "        self.lin2 = Linear(out_channels, 3 * out_channels)\n",
    "        self.lin_rbf = Linear(n_rbf, 3 * out_channels)\n",
    "        self.silu = Func.silu\n",
    "\n",
    "        # self.prepare = Prepare_Message_Vector(num_nodes)\n",
    "        self.RBF = BesselBasis(cut_off, n_rbf)\n",
    "        self.f_cut = CosineCutoff(cut_off)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_feat = num_feat\n",
    "\n",
    "    def forward(self, s, v, s_nuc, v_nuc, edge_index, edge_attr):\n",
    "\n",
    "        s = s.flatten(-1)\n",
    "        v = v.flatten(-2)\n",
    "\n",
    "        s_nuc = s_nuc.flatten(-1)\n",
    "        v_nuc = v_nuc.flatten(-2)\n",
    "\n",
    "        flat_shape_v = v.shape[-1]\n",
    "        flat_shape_s = s.shape[-1]\n",
    "\n",
    "        n_nuc = s_nuc.shape[0]\n",
    "        n_elec = s.shape[0]\n",
    "\n",
    "        x_p = torch.cat([s_nuc, v_nuc], dim=-1)  # nuclei\n",
    "        x = torch.cat([s, v], dim=-1)  # electrons\n",
    "\n",
    "        x = self.propagate(\n",
    "            edge_index,\n",
    "            x=(x_p, x),\n",
    "            edge_attr=edge_attr,\n",
    "            flat_shape_s=flat_shape_s,\n",
    "            flat_shape_v=flat_shape_v,\n",
    "            size=(n_nuc, n_elec),\n",
    "        )\n",
    "\n",
    "        return x\n",
    "\n",
    "    def message(self, x_j, edge_attr, flat_shape_s, flat_shape_v):\n",
    "\n",
    "        # Split Input into s_j and v_j\n",
    "        s_j, v_j = torch.split(x_j, [flat_shape_s, flat_shape_v], dim=-1)\n",
    "        # _, v_i = torch.split(x_i, [flat_shape_s, flat_shape_v], dim=-1)\n",
    "\n",
    "        # r_ij channel\n",
    "        rbf = self.RBF(edge_attr)\n",
    "        ch1 = self.lin_rbf(rbf)\n",
    "        cut = self.f_cut(edge_attr.norm(dim=-1))\n",
    "        W = torch.einsum(\"ij,i->ij\", ch1, cut)  # ch1 * f_cut\n",
    "\n",
    "        # s_j channel\n",
    "        phi = self.lin1(s_j)\n",
    "        phi = self.silu(phi)\n",
    "        phi = self.lin2(phi)\n",
    "\n",
    "        # Split\n",
    "        left, dsm, right = torch.split(phi * W, self.num_feat, dim=-1)\n",
    "\n",
    "        # v_j channel\n",
    "        normalized = Func.normalize(edge_attr, p=2, dim=1)\n",
    "\n",
    "        v_j = v_j.reshape(-1, int(flat_shape_v / 3), 3)\n",
    "        # v_i = v_i.reshape(-1, int(flat_shape_v/3), 3)\n",
    "        # print(v_j - v_i)\n",
    "        hadamard_right = torch.einsum(\"ij,ik->ijk\", right, normalized)\n",
    "        hadamard_left = torch.einsum(\"ijk,ij->ijk\", v_j, left)\n",
    "        dvm = hadamard_left + hadamard_right\n",
    "\n",
    "        # Prepare vector for update\n",
    "        x_j = torch.cat((dsm, dvm.flatten(-2)), dim=-1)\n",
    "\n",
    "        return x_j\n",
    "\n",
    "    def update(self, out_aggr, flat_shape_s, flat_shape_v):\n",
    "\n",
    "        s_j, v_j = torch.split(out_aggr, [flat_shape_s, flat_shape_v], dim=-1)\n",
    "\n",
    "        return s_j, v_j.reshape(-1, int(flat_shape_v / 3), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdatePaiNN(torch.nn.Module):\n",
    "    def __init__(self, num_feat, out_channels, num_nodes):\n",
    "        super(UpdatePaiNN, self).__init__() \n",
    "        \n",
    "        self.lin_up = Linear(2*num_feat, out_channels) \n",
    "        self.denseU = Linear(num_feat,out_channels, bias = False) \n",
    "        self.denseV = Linear(num_feat,out_channels, bias = False) \n",
    "        self.lin2 = Linear(out_channels, 3*out_channels) \n",
    "        self.silu = Func.silu\n",
    "        \n",
    "        \n",
    "    def forward(self, s,v):\n",
    "        \n",
    "        # split and take linear combinations\n",
    "        #s, v = torch.split(out_aggr, [flat_shape_s, flat_shape_v], dim=-1)\n",
    "        \n",
    "        s = s.flatten(-1)\n",
    "        v = v.flatten(-2)\n",
    "        \n",
    "        flat_shape_v = v.shape[-1]\n",
    "        flat_shape_s = s.shape[-1]\n",
    "        \n",
    "        v_u = v.reshape(-1, int(flat_shape_v/3), 3)\n",
    "        v_ut = torch.transpose(v_u,1,2)\n",
    "        U = torch.transpose(self.denseU(v_ut),1,2)\n",
    "        V = torch.transpose(self.denseV(v_ut),1,2)\n",
    "        \n",
    "        \n",
    "        # form the dot product\n",
    "        UV =  torch.einsum('ijk,ijk->ij',U,V) \n",
    "        \n",
    "        # s_j channel\n",
    "        nV = torch.norm(V, dim=-1)\n",
    "\n",
    "        s_u = torch.cat([s, nV], dim=-1)\n",
    "        s_u = self.lin_up(s_u) \n",
    "        s_u = Func.silu(s_u)\n",
    "        s_u = self.lin2(s_u)\n",
    "        #s_u = Func.silu(s_u)\n",
    "        \n",
    "        # final split\n",
    "        top, middle, bottom = torch.tensor_split(s_u,3,dim=-1)\n",
    "        \n",
    "        # outputs\n",
    "        dvu = torch.einsum('ijk,ij->ijk',v_u,top) \n",
    "        dsu = middle*UV + bottom \n",
    "        \n",
    "        #update = torch.cat((dsu,dvu.flatten(-2)), dim=-1)\n",
    "        \n",
    "        return dsu, dvu.reshape(-1, int(flat_shape_v/3), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaiNN(torch.nn.Module):\n",
    "    def __init__(self, num_feat, out_channels, num_nodes, cut_off=5.0, n_rbf=20, num_interactions=3):\n",
    "        super(PaiNN, self).__init__() \n",
    "        '''PyG implementation of PaiNN network of Sch√ºtt et. al. Supports two arrays  \n",
    "           stored at the nodes of shape (num_nodes,num_feat,1) and (num_nodes, num_feat,3). For this \n",
    "           representation to be compatible with PyG, the arrays are flattened and concatenated. \n",
    "           Important to note is that the out_channels must match number of features'''\n",
    "        \n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_interactions = num_interactions\n",
    "        self.cut_off = cut_off\n",
    "        self.n_rbf = n_rbf\n",
    "        self.linear = Linear(num_feat,num_feat)\n",
    "        self.silu = Func.silu\n",
    "        \n",
    "        self.list_message = nn.ModuleList(\n",
    "            [\n",
    "                MessagePassPaiNN(num_feat, out_channels, num_nodes, cut_off, n_rbf)\n",
    "                for _ in range(self.num_interactions)\n",
    "            ]\n",
    "        )\n",
    "        self.list_update = nn.ModuleList(\n",
    "            [\n",
    "                UpdatePaiNN(num_feat, out_channels, num_nodes)\n",
    "                for _ in range(self.num_interactions)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, s,v, edge_index, edge_attr):\n",
    "        \n",
    "        \n",
    "        for i in range(self.num_interactions):\n",
    "            \n",
    "            s_temp,v_temp = self.list_message[i](s,v, edge_index, edge_attr)\n",
    "            s, v = s_temp+s, v_temp+v\n",
    "            s_temp,v_temp = self.list_update[i](s,v) \n",
    "            s, v = s_temp+s, v_temp+v       \n",
    "        \n",
    "        s = self.linear(s)\n",
    "        s = self.silu(s)\n",
    "        s = self.linear(s)\n",
    "        \n",
    "        return v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
