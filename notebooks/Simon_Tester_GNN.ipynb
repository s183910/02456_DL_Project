{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 15:59:43.075273: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-26 15:59:44.534928: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-11-26 15:59:44.534980: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-11-26 15:59:47.603433: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-26 15:59:47.603604: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-26 15:59:47.603616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing record file, delete if you want to re-fetch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 16:00:00.166139: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-11-26 16:00:00.169037: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-11-26 16:00:00.169120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Katrines): /proc/driver/nvidia/version does not exist\n",
      "2023-11-26 16:00:00.173009: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Import functions and load data\n",
    "import os\n",
    "os.chdir(\"../src\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from dataloader import qm9_parse, qm9_fetch\n",
    "import dmol\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "qm9_records = qm9_fetch()\n",
    "data = qm9_parse(qm9_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take samples for test, validation and training\n",
    "shuffled_data = data.shuffle(7000, reshuffle_each_iteration=False)\n",
    "test_set = shuffled_data.take(1000)\n",
    "valid_set = shuffled_data.skip(1000).take(1000)\n",
    "train_set = shuffled_data.skip(2000).take(5000)\n",
    "\n",
    "# for d in test_set:\n",
    "#     print(d)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def convert_record(d, atom_types=100, embedding_dim=128):\n",
    "    # break up record\n",
    "    (e, x), y = d\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    e = torch.tensor(e.numpy())\n",
    "    x = torch.tensor(x.numpy())\n",
    "    r = x[:, :3]\n",
    "\n",
    "    # Assuming atom indices start from 1\n",
    "    e = e - 1\n",
    "    e = torch.clamp(e, 0, atom_types - 1)  # Ensure indices are within valid range\n",
    "\n",
    "    # Embedding\n",
    "    embedding_layer = nn.Embedding(num_embeddings=atom_types, embedding_dim=embedding_dim)\n",
    "    s = embedding_layer(e)\n",
    "\n",
    "    return (s, r), y.numpy()[13]  # Select attribute at index 13\n",
    "\n",
    "\n",
    "#\n",
    "def x2e(x, cutoff_distance=5.0):\n",
    "    \"\"\"convert xyz coordinates to pairwise distance with a cutoff distance\"\"\"\n",
    "   # Calculate pairwise distances\n",
    "   # this calculates the norm\n",
    "    r2 = torch.sqrt(((x - x[:, None, :])**2).sum(dim=-1))\n",
    "\n",
    "    # Create a mask for distances less than cutoff_distance\n",
    "    mask = (r2>0) & (r2 < cutoff_distance)\n",
    "\n",
    "    # Use the mask to set values in the tensor\n",
    "    r_ij = torch.where(mask, r2, torch.zeros_like(r2))\n",
    "\n",
    "    # Generate edge index matrix\n",
    "    #edge_index = torch.nonzero(mask, as_tuple=False)\n",
    "\n",
    "    #edge_mask = (r2 > 0) & (r2 < cutoff_distance)\n",
    "    edge_indices = mask.nonzero(as_tuple=True)\n",
    "    edge_index = torch.stack(edge_indices)\n",
    "    #edge_index = edge_index.resize_(2,len(mask))\n",
    "\n",
    "    return r_ij, edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-320.75858\n",
      "Edge Index: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,\n",
      "          4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,\n",
      "          6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,\n",
      "          9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "         13, 13],\n",
      "        [ 1,  2,  3,  4,  7,  8,  9, 10, 11, 12, 13,  0,  2,  3,  4,  5,  6,  7,\n",
      "          8,  9, 10, 11, 12, 13,  0,  1,  3,  4,  5,  7,  8,  9, 10, 11, 12, 13,\n",
      "          0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0,  1,  2,  3,  5,\n",
      "          6,  7,  8,  9, 10, 11, 12, 13,  1,  2,  3,  4,  6, 10, 11, 12, 13,  1,\n",
      "          3,  4,  5, 10, 11, 12, 13,  0,  1,  2,  3,  4,  8,  9, 10, 11, 12, 13,\n",
      "          0,  1,  2,  3,  4,  7,  9, 10, 11, 12, 13,  0,  1,  2,  3,  4,  7,  8,\n",
      "         10, 11, 12, 13,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 12, 13,  0,\n",
      "          1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13,  0,  1,  2,  3,  4,  5,\n",
      "          6,  7,  8,  9, 10, 11, 13,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,\n",
      "         11, 12]])\n"
     ]
    }
   ],
   "source": [
    "# Assuming test_set is a list of data points\n",
    "for d in test_set:\n",
    "    (s, r_ij), y_raw = convert_record(d)\n",
    "\n",
    "print(y_raw)\n",
    "nodes = e\n",
    "edges = x2e(x)\n",
    "\n",
    "for d in test_set:\n",
    "    (e, x), y = convert_record(d)\n",
    "    r2, edge_index, = x2e(x)\n",
    "    print(\"Edge Index:\", edge_index)\n",
    "    break  # To print only the first molecule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising the targets\n",
    "Is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize y values first and transform after prediction\n",
    "ys = [convert_record(d)[1] for d in train_set]\n",
    "train_ym = np.mean(ys)\n",
    "train_ys = np.std(ys)\n",
    "def transform_label(y):\n",
    "    return (y - train_ym) / train_ys\n",
    "def transform_prediction(y):\n",
    "    return y * train_ys + train_ym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Painn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAINN model\n",
    "\n",
    "from typing import Callable, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import schnetpack.properties as properties\n",
    "# import schnetpack.nn as snn\n",
    "\n",
    "__all__ = [\"phi\", \"RBF\",\"F_cut\",\"w\", \"u\", \"v\",\"S\", \"PaiNN\"]\n",
    "\n",
    "\n",
    "# v_norm = r_ij/torch.sqrt(torch.sum(r_ij**2))\n",
    "\n",
    "# v_j = torch.zeros(128)\n",
    "\n",
    "\n",
    "class phi(nn.Module):\n",
    "     def __init__(self,input_dim, s):\n",
    "        super().__init__()\n",
    "        self.input_dim=128\n",
    "        self.s = s\n",
    "        # self.input_dim=input_dim\n",
    "        activation_fn = nn.SiLU\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 128),\n",
    "            activation_fn(),\n",
    "            nn.Linear(128, 384))\n",
    "\n",
    "    # def forward(self,s):\n",
    "    #     return self.net(s)\n",
    "\n",
    "class RBF(nn.Module):\n",
    "    def __init__(self, r_ij,r_cut):\n",
    "        super().__init__()\n",
    "        self.r_ij = r_ij\n",
    "        self.r_cut = r_cut\n",
    "        self.n_values = torch.arange(1, 21, dtype=torch.float32)\n",
    "    def forward(self,r_ij, r_cut):\n",
    "        r_RBF = torch.sin((self.n_values*torch.pi()/self.r_cut)*r_ij)/r_ij\n",
    "        return r_RBF\n",
    "\n",
    "class F_cut(nn.Module):\n",
    "    def __init__(self, r_ij,r_cut):\n",
    "        super().__init__()\n",
    "        self.r_ij = r_ij\n",
    "        self.r_cut = r_cut\n",
    "    def forward(self, r_ij,r_cut):\n",
    "        f_c=0.5*torch.cos(torch.pi()*r_ij/self.r_cut)+1\n",
    "        return f_c\n",
    "\n",
    "class w(nn.Module):\n",
    "    def __init__(self,r_ij,r_cut):\n",
    "        super().__init__()\n",
    "        self.r_ij = r_ij\n",
    "        self.r_cut = r_cut\n",
    "\n",
    "        self.RBF=RBF(r_ij, 5.0) # mangler vi at importere functionen her??\n",
    "        self.F_cut=F_cut(r_ij,5.0)\n",
    "        self.net = nn.Linear(20,384)\n",
    "\n",
    "    def forward(self,r_ij):\n",
    "        New_RBF = self.RBF(r_ij, 5.0)\n",
    "        New_F_cut=self.F_cut(r_ij, 5.0)\n",
    "        Total = New_RBF*New_F_cut\n",
    "        Output=self.net(Total)\n",
    "        return Output\n",
    "\n",
    "\n",
    "class MessageBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, v_j, s, r_ij):\n",
    "        super().__init__()\n",
    "        # if first pass, # v_j = torch.zeros(128) TODO\n",
    "        self.phi = phi(128, s)\n",
    "        self.w = w(r_ij=None, r_cut=5.0)  # Initialize w with r_ij=None\n",
    "\n",
    "    def forward(self, s, r_ij, v_j, v_norm):\n",
    "        output_phi = self.phi(s)\n",
    "        output_w = self.w(r_ij)\n",
    "        output_conv = output_phi * output_w\n",
    "        output_split = torch.split(output_conv, 3, dim=1)  # Split along the second dimension\n",
    "\n",
    "        # TODO: replace these with edge indexes from x2e\n",
    "        # atom i will be updated as a function of it's j neighbors (atom j)\n",
    "        v_j = torch.zeros(128)\n",
    "        output_v=output_conv[0]*v_j\n",
    "        output_r=output_conv[2]*(r_ij/r_ij) #TODO undersøg denne\n",
    "\n",
    "        delta_s_im=torch.sum(output_conv[1],dim=1)\n",
    "        delta_v_im = torch.sum(output_v+output_r,dim=1)\n",
    "\n",
    "        return delta_s_im, delta_v_im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tester data\n",
    "# epochs=1\n",
    "\n",
    "# for d in train_set:\n",
    "#     (s, r), y_raw = convert_record(d)\n",
    "#     y = transform_label(y_raw)\n",
    "#     r_ij, edge_index = x2e(r, 5.0)\n",
    "# v_j = torch.zeros(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MessageBlock(\n",
       "  (phi): phi(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=128, out_features=384, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (w): w(\n",
       "    (RBF): RBF()\n",
       "    (F_cut): F_cut()\n",
       "    (net): Linear(in_features=20, out_features=384, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tester messageblock\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MessageBlock(v_j, s, r_ij)\n",
    "\n",
    "# delta_s_im, delta_v_im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update block\n",
    "\n",
    "class u(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(128, 128)\n",
    "\n",
    "    def forward(self, v_m):\n",
    "        return self.net(v_m)\n",
    "\n",
    "\n",
    "class v(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(128, 128)\n",
    "\n",
    "    def forward(self, v_m):\n",
    "        return self.net(v_m)\n",
    "\n",
    "\n",
    "class S(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(S, self).__init__()\n",
    "        activation_fn = nn.SiLU()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            activation_fn,\n",
    "            nn.Linear(128, 384)\n",
    "        )\n",
    "\n",
    "    def forward(self, v_norm, s_m):\n",
    "        stack = torch.stack((v_norm, s_m))\n",
    "        output = self.net(stack)\n",
    "        output = torch.split(output, 128)\n",
    "        return output\n",
    "\n",
    "\n",
    "class UpdateBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s_m, self.v_m = MessageBlock() # Kat har added\n",
    "        self.u = u()\n",
    "        self.v = v()\n",
    "        self.s = S()\n",
    "\n",
    "    def forward(self, v_m, v_j, s_j, s_m):\n",
    "        output_u = self.u(v_m)\n",
    "        output_v = self.v(v_m)\n",
    "        output_s = self.s(v_norm=s_m, s_m=s_j)\n",
    "\n",
    "        V_dup = output_v.repeat(1, 2)  # Assuming v_m has shape (batch_size, 128)\n",
    "        output_s1 = output_s[0] * output_u\n",
    "        output_s2 = output_s[1] * V_dup\n",
    "        output_s3 = output_s[2] + output_s2\n",
    "\n",
    "        # TODO: replace these with edge indexes from x2e\n",
    "        # atom i will be updated as a function of it's j neighbors (atom j)\n",
    "\n",
    "\n",
    "        v_i = output_s1 + v_j\n",
    "        s_i = output_s3 + s_j\n",
    "\n",
    "        return v_i, s_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Painn model\n",
    "\n",
    "Not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final PAINN model\n",
    "\n",
    "# TODO: fix painn modellen\n",
    "# KAT her\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.message_block = MessageBlock()\n",
    "        self.update_block = UpdateBlock()\n",
    "\n",
    "\n",
    "        # You may need to define an initial input, for example, s_j, r_ij, v_j, and v_norm\n",
    "        # Replace the following with your actual initialization logic\n",
    "        self.initial_s_j = torch.zeros((batch_size, 128))\n",
    "        self.initial_r_ij = torch.zeros((batch_size, 3))\n",
    "        self.initial_v_j = torch.zeros((batch_size, 128))\n",
    "        self.initial_v_norm = torch.zeros((batch_size, 3))\n",
    "\n",
    "    def forward(self, num_iterations):\n",
    "        # Initialize variables\n",
    "        s_j = self.initial_s_j\n",
    "        r_ij = self.initial_r_ij\n",
    "        v_j = self.initial_v_j\n",
    "        v_norm = self.initial_v_norm\n",
    "\n",
    "        for _ in range(num_iterations):\n",
    "            # Message block\n",
    "            output1 = self.message_block.phi(s_j)\n",
    "            output2 = self.message_block.w(r_ij)\n",
    "            output = output1 * output2\n",
    "            output_split = torch.split(output, 3, dim=1)\n",
    "\n",
    "            # Update s_m\n",
    "            s_m = torch.sum(output_split[1], dim=1, keepdim=True) + s_j\n",
    "\n",
    "            # Update v_m\n",
    "            output3 = output_split[2] * v_norm\n",
    "            v_m = torch.sum(output3, dim=1, keepdim=True) + v_j\n",
    "\n",
    "            # Update block\n",
    "            output_u = self.update_block.u(v_m)\n",
    "            output_v = self.update_block.v(v_m)\n",
    "            output_s = self.update_block.s(v_norm=s_m, s_m=s_j)\n",
    "\n",
    "            V_dup = output_v.repeat(1, 2)\n",
    "            output_s1 = output_s[0] * output_u\n",
    "            output_s2 = output_s[1] * V_dup\n",
    "            output_s3 = output_s[2] + output_s2\n",
    "\n",
    "            v_i = output_s1 + v_j\n",
    "            s_i = output_s3 + s_j\n",
    "\n",
    "            # Update variables for the next iteration\n",
    "            s_j = s_i\n",
    "            v_j = v_i\n",
    "\n",
    "        # The final v_i and s_i after all iterations\n",
    "        return v_i, s_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final PAINN model - Katrine\n",
    "\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.message_block = MessageBlock()\n",
    "        self.update_block = UpdateBlock()\n",
    "\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    def __init__(self, phi_input_size, r_ij, r_cut, v_m_size, s_m_size):\n",
    "        super().__init__()\n",
    "        self.message_block = Message_block(phi_input_size, r_ij, r_cut)\n",
    "        self.update_block = Update_block(v_m_size, s_m_size)\n",
    "\n",
    "\n",
    "    def forward(self, input1, input2, v_j, s, v_norm):\n",
    "        # Forward pass through the message block\n",
    "        v_m, s_m = self.message_block(input1, input2, v_j, s, v_norm)\n",
    "\n",
    "        # Forward pass through the update block\n",
    "        v_u, s_u = self.update_block(v_m, v_m, s_m, v_m, s_m)\n",
    "\n",
    "        # Return the updated values\n",
    "        return v_u, s_u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to iterate through the data\n",
    "eta = 1e-3\n",
    "val_loss = [0.0 for _ in range(epochs)]\n",
    "epochs=3\n",
    "for epoch in range(epochs):\n",
    "    for d in train_set:\n",
    "        (e, x), y_raw = convert_record(d)\n",
    "        y = transform_label(y_raw)\n",
    "        grad = loss_grad(e, x, y, w1, w2, w3, b)\n",
    "\n",
    "\n",
    "\n",
    "        # TODO: Look at this, this is made by the chat\n",
    "        # update regression weights\n",
    "        w3 -= eta * grad[2]\n",
    "        b -= eta * grad[3]\n",
    "        # update GNN weights\n",
    "        for i, w in [(0, w1), (1, w2)]:\n",
    "            for j in range(len(w)):\n",
    "                w[j] -= eta * grad[i][j] / 10\n",
    "    # compute validation loss\n",
    "    for v in valid_set:\n",
    "        (e, x), y_raw = convert_record(v)\n",
    "        y = transform_label(y_raw)\n",
    "        # convert SE to RMSE\n",
    "        val_loss[epoch] += loss(e, x, y, w1, w2, w3, b)\n",
    "    val_loss[epoch] = jnp.sqrt(val_loss[epoch] / 1000)\n",
    "    eta *= 0.9\n",
    "plt.plot(baseline_val_loss, label=\"baseline\")\n",
    "plt.plot(val_loss, label=\"GNN\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Val Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = []\n",
    "yhats = []\n",
    "for v in valid_set:\n",
    "    (e, x), y = convert_record(v)\n",
    "    ys.append(y)\n",
    "    yhat_raw = model(e, x, w1, w2, w3, b)\n",
    "    yhats.append(transform_prediction(yhat_raw))\n",
    "\n",
    "\n",
    "plt.plot(ys, ys, \"-\")\n",
    "plt.plot(ys, yhats, \".\")\n",
    "plt.xlabel(\"Energy\")\n",
    "plt.ylabel(\"Predicted Energy\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
