{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing record file, delete if you want to re-fetch\n"
     ]
    }
   ],
   "source": [
    "#Import functions and load data\n",
    "import os\n",
    "os.chdir(\"../src\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from dataloader import qm9_parse, qm9_fetch\n",
    "import dmol\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "qm9_records = qm9_fetch()\n",
    "data = qm9_parse(qm9_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take samples for test, validation and training\n",
    "shuffled_data = data.shuffle(7000, reshuffle_each_iteration=False)\n",
    "test_set = shuffled_data.take(1000)\n",
    "valid_set = shuffled_data.skip(1000).take(1000)\n",
    "train_set = shuffled_data.skip(2000).take(5000)\n",
    "\n",
    "# for d in test_set:\n",
    "#     print(d)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def convert_record(d, atom_types=100, embedding_dim=128):\n",
    "    # break up record\n",
    "    (e, x), y = d\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    e = torch.tensor(e.numpy())\n",
    "    x = torch.tensor(x.numpy())\n",
    "    r = x[:, :3]\n",
    "\n",
    "    # Assuming atom indices start from 1\n",
    "    e = e - 1\n",
    "    e = torch.clamp(e, 0, atom_types - 1)  # Ensure indices are within valid range\n",
    "\n",
    "    # Embedding\n",
    "    embedding_layer = nn.Embedding(num_embeddings=atom_types, embedding_dim=embedding_dim)\n",
    "    s = embedding_layer(e)\n",
    "\n",
    "    return (s, r), y.numpy()[13]  # Select attribute at index 13\n",
    "\n",
    "\n",
    "#\n",
    "def x2e(x, cutoff_distance=5.0):\n",
    "    \"\"\"convert xyz coordinates to pairwise distance with a cutoff distance\"\"\"\n",
    "   # Calculate pairwise distances\n",
    "   # this calculates the norm\n",
    "    r2 = torch.sqrt(((x - x[:, None, :])**2).sum(dim=-1))\n",
    "\n",
    "    # Create a mask for distances less than cutoff_distance\n",
    "    mask = (r2>0) & (r2 < cutoff_distance)\n",
    "\n",
    "    # Use the mask to set values in the tensor\n",
    "    r_ij = torch.where(mask, r2, torch.zeros_like(r2))\n",
    "\n",
    "    # Generate edge index matrix\n",
    "    #edge_index = torch.nonzero(mask, as_tuple=False)\n",
    "\n",
    "    #edge_mask = (r2 > 0) & (r2 < cutoff_distance)\n",
    "    edge_indices = mask.nonzero(as_tuple=True)\n",
    "    edge_index = torch.stack(edge_indices)\n",
    "    #edge_index = edge_index.resize_(2,len(mask))\n",
    "\n",
    "    return r_ij, edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Index: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5,\n",
      "         5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8,\n",
      "         8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
      "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 3, 4, 5, 6,\n",
      "         7, 8, 9, 0, 1, 2, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 5, 6, 7, 8, 9, 0, 1, 2,\n",
      "         3, 4, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 7, 9, 0, 1, 2, 3, 4, 5, 6, 8, 9, 0,\n",
      "         1, 2, 3, 4, 5, 7, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# Assuming test_set is a list of data points\n",
    "for d in test_set:\n",
    "    (s, r_ij), y_raw = convert_record(d)\n",
    "\n",
    "for d in test_set:\n",
    "    (e, x), y = convert_record(d)\n",
    "    r2, edge_index, = x2e(x)\n",
    "    print(\"Edge Index:\", edge_index)\n",
    "    break  # To print only the first molecule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising the targets\n",
    "Is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize y values first and transform after prediction\n",
    "ys = [convert_record(d)[1] for d in train_set]\n",
    "train_ym = np.mean(ys)\n",
    "train_ys = np.std(ys)\n",
    "def transform_label(y):\n",
    "    return (y - train_ym) / train_ys\n",
    "def transform_prediction(y):\n",
    "    return y * train_ys + train_ym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Painn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Message block\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class phi(nn.Module):\n",
    "    def __init__(self, input_dim=128):\n",
    "        super().__init__()\n",
    "        activation_fn = nn.SiLU\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim, bias=False),\n",
    "            activation_fn(),\n",
    "            nn.Linear(input_dim, 384, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.net(s)\n",
    "\n",
    "class RBF(nn.Module):\n",
    "    def __init__(self, r_cut=5.0):\n",
    "        super().__init__()\n",
    "        self.r_cut = r_cut\n",
    "        self.n_values = torch.arange(1, 21, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, r_ij):\n",
    "        r_RBF_list = []\n",
    "\n",
    "        for n_value in self.n_values:\n",
    "            r_RBF_n = (torch.sin((n_value * 3.14 / self.r_cut) * r_ij)) / r_ij\n",
    "            r_RBF_list.append(r_RBF_n)\n",
    "\n",
    "        r_RBF = torch.stack(r_RBF_list, dim=1)\n",
    "        return r_RBF\n",
    "\n",
    "class F_cut(nn.Module):\n",
    "    def __init__(self, r_cut=5.0):\n",
    "        super().__init__()\n",
    "        self.r_cut = r_cut\n",
    "\n",
    "    def forward(self, r_ij):\n",
    "        f_c = 0.5 * torch.cos(torch.pi * r_ij / self.r_cut) + 1\n",
    "        return f_c\n",
    "\n",
    "class w(nn.Module):\n",
    "    def __init__(self, r_cut=5.0):\n",
    "        super().__init__()\n",
    "        self.RBF = RBF(r_cut)\n",
    "        self.F_cut = F_cut(r_cut)\n",
    "        self.net = nn.Linear(20, 384, bias=False)\n",
    "\n",
    "    def forward(self, r_ij):\n",
    "        New_RBF = self.RBF(r_ij)\n",
    "        New_F_cut = self.F_cut(r_ij).unsqueeze(1)\n",
    "        Total = New_RBF * New_F_cut\n",
    "        Output = self.net(Total)\n",
    "        return Output\n",
    "\n",
    "class MessageBlock(nn.Module):\n",
    "    def __init__(self, input_dim=128):\n",
    "        super().__init__()\n",
    "        self.phi = phi(input_dim)\n",
    "        self.w = w()\n",
    "        self.v_j = nn.Parameter(torch.zeros(input_dim))\n",
    "\n",
    "    def forward(self, v_j, s, r_ij):\n",
    "        output_phi = self.phi(s)\n",
    "        output_w = self.w(r_ij)\n",
    "        output_conv = output_phi * output_w\n",
    "        output_split = torch.chunk(output_conv, 3, dim=1)\n",
    "\n",
    "        output_v = output_split[0] * v_j  # Select the first 128 elements\n",
    "        delta_s_im = output_split[1]  # Select the next 128 elements\n",
    "        output_r = output_split[2] * (r_ij / r_ij)  # Select the last 128 elements #TODO: check norm\n",
    "\n",
    "        delta_s_im = torch.sum(delta_s_im, dim=1)\n",
    "        delta_v_im = torch.sum(output_v + output_r, dim=1)\n",
    "\n",
    "        return delta_s_im, delta_v_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tester data\n",
    "# epochs=1\n",
    "\n",
    "# for d in train_set:\n",
    "#     (s, r), y_raw = convert_record(d)\n",
    "#     y = transform_label(y_raw)\n",
    "#     r_ij, edge_index = x2e(r, 5.0)\n",
    "# v_j = torch.zeros(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.3254e-01,  9.0767e+00, -1.2743e+00,  5.8919e+00,  3.1333e-01,\n",
      "         3.4701e+00,  6.3691e+00, -2.1632e+00, -3.8876e-01,  9.3376e+00,\n",
      "        -1.0047e+00, -1.5810e+00, -6.1071e-01,  8.9483e+00, -1.2543e+00,\n",
      "        -1.3096e+00, -4.0785e+00, -2.2005e-01,  6.1415e+00, -1.1116e+00,\n",
      "         4.6796e-01, -2.5426e+00,  8.0881e+00, -9.3868e-01, -4.3547e-01,\n",
      "        -3.8047e+00, -7.5829e-01, -9.1429e-01,  1.1912e+00, -2.6474e+00,\n",
      "        -2.5624e+00, -1.7802e+00, -4.0688e-01,  4.2336e-01, -3.3661e+00,\n",
      "        -8.4621e-01,  3.6178e-01, -3.4372e+00, -3.9461e-01,  4.8404e-01,\n",
      "        -3.9874e+00,  8.7018e+00,  1.7931e-01, -4.0286e+00, -3.8520e+00,\n",
      "        -3.2793e+00,  7.3267e+00, -1.8629e+00,  2.8674e+00, -3.3149e+00,\n",
      "         3.7587e-01,  9.4984e+00, -1.0635e+00, -4.9327e-01, -1.1530e+00,\n",
      "        -1.1500e+00, -1.5754e+00, -4.5232e-01,  9.4383e+00,  8.1370e+00,\n",
      "         6.7399e+00, -5.4844e-01, -2.1110e+00, -3.1753e-01,  4.8998e+00,\n",
      "         5.8828e+00, -1.2686e+00, -1.6562e-01, -3.7893e+00,  7.4520e+00,\n",
      "         1.5832e+00, -2.1401e+00, -1.1611e+00,  4.1606e+00, -3.8764e+00,\n",
      "         8.6916e-03, -4.0755e+00, -2.7959e-01, -1.2230e-01, -2.8934e+00,\n",
      "        -1.0051e+00, -9.3828e-01, -4.0752e+00, -1.7490e-01, -2.8744e+00,\n",
      "         3.7337e-01, -9.1867e-01, -3.8255e+00, -4.0684e+00, -9.2829e-01,\n",
      "        -1.1053e+00, -6.1249e-01, -9.7660e-01, -5.4949e-01, -3.8552e+00,\n",
      "         8.1144e+00,  3.1814e-01,  2.6741e+00, -1.3098e+00, -3.8880e-01,\n",
      "         4.3796e+00, -1.2802e+00, -4.9605e-01, -3.4803e+00, -3.4195e-01,\n",
      "        -2.6506e+00,  2.7177e-01, -1.5686e+00,  9.0699e+00, -1.2239e+00,\n",
      "         4.9886e-01,  9.1963e+00, -8.5754e-01, -9.2183e-01, -1.8770e+00,\n",
      "         8.5119e+00,  9.5151e+00,  9.5971e+00, -6.4515e-01,  4.7222e-01,\n",
      "        -3.8323e+00, -3.6933e+00, -4.0767e+00, -3.9248e+00,  6.9927e+00,\n",
      "        -7.1312e-01, -9.6631e-01,  4.8940e-01], grad_fn=<SumBackward1>) tensor([-1.7043e+00, -1.1075e+01, -9.3292e-01, -4.2926e+00,  9.3196e-01,\n",
      "        -1.4386e+00, -5.0756e+00, -2.3481e+00, -1.5753e+00, -1.1807e+01,\n",
      "         2.9099e+00, -5.4387e-01, -5.8542e-01, -1.0725e+01, -6.4083e-01,\n",
      "         5.2030e-01, -5.2675e+00, -2.2771e+00, -4.6926e+00, -1.4315e+00,\n",
      "         3.8639e-01, -3.3257e+00, -8.5592e+00,  2.4271e+00, -2.7662e-01,\n",
      "        -5.5119e+00, -6.4967e-01,  2.8660e+00, -3.5402e-01, -3.5719e+00,\n",
      "        -3.3729e+00, -1.2103e+00, -5.0847e-01, -1.0424e+00, -3.2638e+00,\n",
      "        -2.0732e+00, -4.7054e-01, -3.3935e+00, -2.3439e+00, -6.3920e-01,\n",
      "        -5.5611e+00, -1.0074e+01, -1.7732e+00, -5.5262e+00, -5.5421e+00,\n",
      "        -4.8372e+00, -6.8835e+00, -1.4705e+00, -1.0044e+00, -3.1738e+00,\n",
      "        -8.5914e-01, -1.2274e+01,  2.7328e+00, -2.3431e+00,  2.3223e+00,\n",
      "         2.3389e+00, -5.2434e-01, -1.4441e+00, -1.2098e+01, -8.6744e+00,\n",
      "        -5.7374e+00, -5.6047e-01, -2.2025e+00,  5.6541e-02, -2.9029e+00,\n",
      "        -4.2783e+00,  1.4090e+00, -4.3472e-01, -4.1464e+00, -7.1444e+00,\n",
      "        -4.3279e-01, -2.2842e+00,  1.1285e+00, -2.0684e+00, -4.3779e+00,\n",
      "        -2.0557e+00, -5.2080e+00,  1.3506e-01,  3.7087e-01, -4.1104e+00,\n",
      "         1.9499e+00,  2.4308e+00, -5.2037e+00, -2.2458e+00, -2.4935e+00,\n",
      "        -1.2505e+00,  2.9166e+00, -4.2392e+00, -5.1346e+00, -1.9320e+00,\n",
      "         2.5623e+00, -2.3025e+00,  2.1343e+00, -2.3296e+00, -4.3188e+00,\n",
      "        -8.6211e+00,  9.2162e-01, -8.8766e-01,  3.3709e-01, -1.5753e+00,\n",
      "        -2.2982e+00, -3.7036e-01, -2.3427e+00, -3.4750e+00, -1.6352e+00,\n",
      "        -2.2000e+00,  1.0125e+00, -5.0026e-01, -1.1056e+01, -9.0111e-01,\n",
      "        -1.4771e-01, -1.1407e+01, -6.9699e-01,  2.6049e+00, -1.5138e+00,\n",
      "        -9.5889e+00, -1.2323e+01, -1.2566e+01, -5.9976e-01, -5.1539e-03,\n",
      "        -5.5306e+00, -3.9177e+00, -5.2265e+00, -5.5687e+00, -6.2161e+00,\n",
      "        -2.2307e+00, -1.8524e+00, -5.7377e-01], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# tester messageblock\n",
    "\n",
    "#MessageBlock(v_j, s, r_ij)\n",
    "v_j = torch.randn(128)\n",
    "s = torch.randn(128)\n",
    "r_ij = torch.randn(128)\n",
    "\n",
    "delta_s_im, delta_v_im = MessageBlock(input_dim=128).forward(v_j, s, r_ij)\n",
    "\n",
    "print(delta_s_im,delta_v_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update block\n",
    "\n",
    "class u(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(128, 128,bias=False)\n",
    "\n",
    "    def forward(self, v_m):\n",
    "        u_m = self.net(v_m)\n",
    "        return u_m\n",
    "\n",
    "\n",
    "class v(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(128, 128,bias=False)\n",
    "\n",
    "    def forward(self, v_m):\n",
    "        return self.net(v_m)\n",
    "\n",
    "\n",
    "class S(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        activation_fn = nn.SiLU()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            activation_fn,\n",
    "            nn.Linear(128, 384)\n",
    "        )\n",
    "\n",
    "    def forward(self, v_m, s_m):\n",
    "        stack = torch.stack((torch.norm(v_m), s_m)) #TODO: stacking of a norm and a tensor?\n",
    "        output = self.net(stack)\n",
    "        return output\n",
    "\n",
    "\n",
    "class UpdateBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.s_m, self.v_m = MessageBlock() # Kat har added\n",
    "        self.u = u()\n",
    "        self.v = v()\n",
    "        self.s = S()\n",
    "\n",
    "    def forward(self, v_m, s_m):\n",
    "        output_u = self.u(v_m)\n",
    "        output_v = self.v(v_m)\n",
    "        output_s = self.s(v_m, s_m)\n",
    "\n",
    "        output_s = torch.chunk(output_s, 3, dim=1)\n",
    "        V_dup = output_v.repeat(1, 2)  # Assuming v_m has shape (batch_size, 128)\n",
    "        output_s1 = output_s[0] * output_u\n",
    "        output_s2 = output_s[1] * V_dup.squeeze(1)\n",
    "        output_s3 = output_s[2] + output_s2\n",
    "\n",
    "        delta_v_iu = output_s1\n",
    "        delta_s_iu = output_s2 + output_s3\n",
    "        # TODO: replace these with edge indexes from x2e\n",
    "        # atom i will be updated as a function of it's j neighbors (atom j)\n",
    "\n",
    "        return delta_v_iu, delta_s_iu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [] at entry 0 and [128] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vonge\\OneDrive\\Dokumenter\\GitHub\\02456_DL_Project\\notebooks\\Simon_Tester_GNN.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m s_m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m128\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m v_m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m128\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m delta_v_iu, delta_s_iu \u001b[39m=\u001b[39m UpdateBlock()\u001b[39m.\u001b[39;49mforward(v_m,s_m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(delta_s_im,delta_v_im)\n",
      "\u001b[1;32mc:\\Users\\vonge\\OneDrive\\Dokumenter\\GitHub\\02456_DL_Project\\notebooks\\Simon_Tester_GNN.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m output_u \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu(v_m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m output_v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv(v_m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m output_s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ms(v_m, s_m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m output_s \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mchunk(output_s, \u001b[39m3\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m V_dup \u001b[39m=\u001b[39m output_v\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# Assuming v_m has shape (batch_size, 128)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\vonge\\OneDrive\\Dokumenter\\GitHub\\02456_DL_Project\\notebooks\\Simon_Tester_GNN.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, v_m, s_m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     stack \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack((torch\u001b[39m.\u001b[39;49mnorm(v_m), s_m)) \u001b[39m#TODO: stacking of a norm and a tensor?\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(stack)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/notebooks/Simon_Tester_GNN.ipynb#X56sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [] at entry 0 and [128] at entry 1"
     ]
    }
   ],
   "source": [
    "#Testing update block\n",
    "\n",
    "s_m = torch.randn(128)\n",
    "v_m = torch.randn(128)\n",
    "\n",
    "delta_v_iu, delta_s_iu = UpdateBlock().forward(v_m,s_m) #TODO: modellen kan ikke finde v_m og s_m eftersom de ikker defineret endnu\n",
    "\n",
    "print(delta_s_im,delta_v_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Painn model\n",
    "\n",
    "Not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final PAINN model\n",
    "\n",
    "# TODO: fix painn modellen\n",
    "# KAT her\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    def __init__(self, message_block, update_block):\n",
    "        super().__init__()\n",
    "        self.message_block = message_block\n",
    "        self.update_block = update_block\n",
    "\n",
    "    def forward(self, v_j, s, r_ij, num_iterations):\n",
    "        for _ in range(num_iterations):\n",
    "            # Message block\n",
    "            output1 = self.message_block.phi(s_j)\n",
    "            output2 = self.message_block.w(r_ij)\n",
    "            output = output1 * output2\n",
    "            output_split = torch.split(output, 3, dim=1)\n",
    "\n",
    "            # Update s_m\n",
    "            s_m = torch.sum(output_split[1], dim=1, keepdim=True) + s_j\n",
    "\n",
    "            # Update v_m\n",
    "            output3 = output_split[2] * v_norm\n",
    "            v_m = torch.sum(output3, dim=1, keepdim=True) + v_j\n",
    "\n",
    "\n",
    "            v_i = output_s1 + v_j\n",
    "            s_i = output_s3 + s_j\n",
    "\n",
    "            # Update variables for the next iteration\n",
    "            s_j = s_i\n",
    "            v_j = v_i\n",
    "\n",
    "        # The final v_i and s_i after all iterations\n",
    "        return v_i, s_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final PAINN model - Katrine\n",
    "\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.message_block = MessageBlock()\n",
    "        self.update_block = UpdateBlock()\n",
    "\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    def __init__(self, phi_input_size, r_ij, r_cut, v_m_size, s_m_size):\n",
    "        super().__init__()\n",
    "        self.message_block = MessageBlock(phi_input_size, r_ij, r_cut)\n",
    "        self.update_block = UpdateBlock(v_m_size, s_m_size)\n",
    "\n",
    "\n",
    "    def forward(self, input1, input2, v_j, s, v_norm):\n",
    "        # Forward pass through the message block\n",
    "        v_m, s_m = self.message_block(input1, input2, v_j, s, v_norm)\n",
    "\n",
    "        # Forward pass through the update block\n",
    "        v_u, s_u = self.update_block(v_m, v_m, s_m, v_m, s_m)\n",
    "\n",
    "        # Return the updated values\n",
    "        return v_u, s_u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to iterate through the data\n",
    "eta = 1e-3\n",
    "val_loss = [0.0 for _ in range(epochs)]\n",
    "epochs=3\n",
    "for epoch in range(epochs):\n",
    "    for d in train_set:\n",
    "        (e, x), y_raw = convert_record(d)\n",
    "        y = transform_label(y_raw)\n",
    "        grad = loss_grad(e, x, y, w1, w2, w3, b)\n",
    "\n",
    "\n",
    "\n",
    "        # TODO: Look at this, this is made by the chat\n",
    "        # update regression weights\n",
    "        w3 -= eta * grad[2]\n",
    "        b -= eta * grad[3]\n",
    "        # update GNN weights\n",
    "        for i, w in [(0, w1), (1, w2)]:\n",
    "            for j in range(len(w)):\n",
    "                w[j] -= eta * grad[i][j] / 10\n",
    "    # compute validation loss\n",
    "    for v in valid_set:\n",
    "        (e, x), y_raw = convert_record(v)\n",
    "        y = transform_label(y_raw)\n",
    "        # convert SE to RMSE\n",
    "        val_loss[epoch] += loss(e, x, y, w1, w2, w3, b)\n",
    "    val_loss[epoch] = jnp.sqrt(val_loss[epoch] / 1000)\n",
    "    eta *= 0.9\n",
    "plt.plot(baseline_val_loss, label=\"baseline\")\n",
    "plt.plot(val_loss, label=\"GNN\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Val Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = []\n",
    "yhats = []\n",
    "for v in valid_set:\n",
    "    (e, x), y = convert_record(v)\n",
    "    ys.append(y)\n",
    "    yhat_raw = model(e, x, w1, w2, w3, b)\n",
    "    yhats.append(transform_prediction(yhat_raw))\n",
    "\n",
    "\n",
    "plt.plot(ys, ys, \"-\")\n",
    "plt.plot(ys, yhats, \".\")\n",
    "plt.xlabel(\"Energy\")\n",
    "plt.ylabel(\"Predicted Energy\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
