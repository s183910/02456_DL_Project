{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../src\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from dataloader import qm9_parse, qm9_fetch\n",
    "import dmol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing record file, delete if you want to re-fetch\n"
     ]
    }
   ],
   "source": [
    "qm9_records = qm9_fetch()\n",
    "\n",
    "data = qm9_parse(qm9_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in data:\n",
    "#     print(d)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train with 5000 and use 1000 for test\n",
    "shuffled_data = data.shuffle(700, reshuffle_each_iteration=False)\n",
    "test_set = shuffled_data.take(100)\n",
    "valid_set = shuffled_data.skip(100).take(100)\n",
    "train_set = shuffled_data.skip(200).take(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<tf.Tensor: shape=(9,), dtype=int64, numpy=array([8, 6, 6, 6, 8, 1, 1, 1, 1])>, <tf.Tensor: shape=(9, 4), dtype=float32, numpy=\n",
      "array([[ 0.04424004,  0.03933243, -0.33071104, -0.308076  ],\n",
      "       [-0.04854479,  1.1741623 ,  0.00410076,  0.452632  ],\n",
      "       [-1.1421165 ,  2.2361186 ,  0.15338361, -0.400942  ],\n",
      "       [ 0.02332538,  3.1235702 ,  0.6168162 , -0.072585  ],\n",
      "       [ 0.9566381 ,  2.0119011 ,  0.42603746, -0.231459  ],\n",
      "       [-1.6257375 ,  2.514822  , -0.784011  ,  0.149267  ],\n",
      "       [-1.9000921 ,  1.9955338 ,  0.9002866 ,  0.149263  ],\n",
      "       [ 0.28368077,  3.9617276 , -0.03229287,  0.13095   ],\n",
      "       [ 0.0078225 ,  3.4394267 ,  1.6616273 ,  0.13095   ]],\n",
      "      dtype=float32)>), <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([ 9.2000000e+01,  1.2446430e+01,  5.2332802e+00,  3.8659899e+00,\n",
      "        3.9339001e+00,  3.5400002e+01, -2.7880001e-01,  8.8999998e-03,\n",
      "        2.8780001e-01,  3.3607919e+02,  6.8573996e-02, -2.6710620e+02,\n",
      "       -2.6710193e+02, -2.6710098e+02, -2.6713333e+02,  1.4483000e+01],\n",
      "      dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for d in train_set:\n",
    "    print(d)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the embedding layer outside the loop\n",
    "embedding_layer = nn.Embedding(16, 128)\n",
    "\n",
    "# def convert_record(d):\n",
    "#     # break up record\n",
    "#     (e, x), y = d\n",
    "#     e = e.numpy()\n",
    "#     x= x.numpy()\n",
    "#     r= x[:,:3]\n",
    "#     # embedding= nn.Embedding(100,128,padding_idx=0)\n",
    "#     e=torch.tensor(e)\n",
    "#     e = embedding(e)\n",
    "#     r =torch.tensor(r)\n",
    "#     return(e,r),y.numpy()[13]\n",
    "\n",
    "# for d in data:\n",
    "#     print(d)\n",
    "#     (e, x), y = convert_record(d)\n",
    "#     print(\"Embedded Atomic Numbers:\\n\", e)\n",
    "#     print(\"Coordinates:\\n\", x)\n",
    "#     print(\"Label:\", y)\n",
    "#     print()\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_record(d):\n",
    "    # break up record\n",
    "    (e, x), y = d\n",
    "    e = e.numpy()\n",
    "    x= x.numpy()\n",
    "    r= x[:,:3]\n",
    "    embedding= nn.Embedding(100,128,padding_idx=0)\n",
    "    e=torch.tensor(e)\n",
    "    s_j = embedding(e)\n",
    "    r =torch.tensor(r)\n",
    "    return(s_j,r),y.numpy()[13]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x2e(x):\n",
    "    \"\"\"convert xyz coordinates to inverse pairwise distance\"\"\"\n",
    "    r2 = torch.sum((x - x[:, None, :]) ** 2, axis=-1)\n",
    "    # r2 = jnp.sum((x - x[:, jnp.newaxis, :]) ** 2, axis=-1)\n",
    "    # e = jnp.where(r2 != 0, 1 / r2, 0.0)\n",
    "    e = torch.sqrt(torch.where(r2 != 0, r2, 0.0))\n",
    "\n",
    "    return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = train_set\n",
    "for d in data_type:\n",
    "    (s_j,r),response_var = convert_record(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0600, 1.5369, 0.0097])\n",
      "tensor([ 1.3241,  0.7053, -0.0068])\n",
      "tensor([-0.0171,  0.0431,  0.0025])\n",
      "tensor([-0.6194, -0.8931,  1.0450])\n",
      "tensor([-1.1798, -1.7240,  0.0041])\n",
      "tensor([-0.6393, -0.8819, -1.0383])\n",
      "tensor([-0.1994,  2.0575,  0.9265])\n",
      "tensor([-0.2168,  2.0672, -0.8963])\n",
      "tensor([ 1.9041,  0.6720, -0.9240])\n",
      "tensor([1.9215, 0.6622, 0.8988])\n",
      "tensor([ 0.0969, -1.4442,  1.6694])\n",
      "tensor([-1.3974, -0.4610,  1.6891])\n",
      "tensor([-1.4294, -0.4430, -1.6627])\n",
      "tensor([ 0.0648, -1.4262, -1.6822])\n"
     ]
    }
   ],
   "source": [
    "for atom in r:\n",
    "    print(atom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_r_ij(r, r_cut=5.0):\n",
    "#     r_ij = torch.tensor([])\n",
    "#     for i in range(len(r)):\n",
    "#         for j in range(len(r)):\n",
    "#             if torch.norm(r[i] - r[j]) <r_cut:\n",
    "#                 r_ij = torch.cat((r_ij, r[i] - r[j])) #important\n",
    "#             else:\n",
    "#                 continue\n",
    "#         return r_ij\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0.]),\n",
       " tensor([-1.2641,  0.8317,  0.0165]),\n",
       " tensor([0.0771, 1.4938, 0.0073]),\n",
       " tensor([ 0.6794,  2.4300, -1.0352]),\n",
       " tensor([1.2398, 3.2609, 0.0057]),\n",
       " tensor([0.6993, 2.4188, 1.0481]),\n",
       " tensor([ 0.2594, -0.5206, -0.9168]),\n",
       " tensor([ 0.2768, -0.5303,  0.9060]),\n",
       " tensor([-1.8441,  0.8650,  0.9338]),\n",
       " tensor([-1.8615,  0.8748, -0.8890]),\n",
       " tensor([-0.0369,  2.9811, -1.6597]),\n",
       " tensor([ 1.4574,  1.9980, -1.6793]),\n",
       " tensor([1.4894, 1.9800, 1.6725]),\n",
       " tensor([-0.0048,  2.9631,  1.6920])]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vi bruger denne\n",
    "def create_r_ij(r, r_cut=5.0):\n",
    "    r_ij = []\n",
    "    for i in range(len(r)):\n",
    "        for j in range(len(r)):\n",
    "            if torch.norm(r[i] - r[j]) <r_cut:\n",
    "                r_ij.append(r[i] - r[j]) #important\n",
    "            else:\n",
    "                continue\n",
    "        return r_ij\n",
    "create_r_ij(r,r_cut=5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input\n",
    "\n",
    "r_ij = create_r_ij(r,r_cut=5.0) #vectorization\n",
    "A = x2e(r) # adjacency matrix\n",
    "r_pos = r # position matrix\n",
    "s_j = s_j\n",
    "r_cut = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ij_tensor = torch.stack(r_ij)\n",
    "r_ij_norm  = torch.norm(r_ij_tensor)\n",
    "v_norm = r_ij_tensor/(torch.norm(r_ij_tensor))\n",
    "v_j = torch.zeros(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_ij_norm = r_ij_norm.item()\n",
    "type(r_ij_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.898587196589413e-16"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sin(20*np.pi/r_cut)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kat tester vores netvÃ¦rk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Inputs som skal fjernes\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "\n",
    "__all__ = [\"phi\", \"w\", \"u\", \"v\", \"s\", \"Message_block\", \"Update_block\", \"PaiNN\"]\n",
    "\n",
    "class phi(nn.Module):\n",
    "    def __init__(self, s_j):\n",
    "        super().__init__()\n",
    "        activation_fn = nn.SiLU\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_j, 128),\n",
    "            activation_fn(),\n",
    "            nn.Linear(128, 384),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# class w(nn.Module):\n",
    "#     def __init__(self, r_ij_norm, r_cut ):\n",
    "#         super().__init__()\n",
    "#         N = [1,2,3,4,5,6,7,8,9,10]\n",
    "#         RBF = []\n",
    "#         for n in N:#range(1,21):\n",
    "#             # if n>0: # hvorfor er denne her???? Hvad er i?\n",
    "#             RBF.append(np.sin((n*np.pi/r_cut)*r_ij_norm)/r_ij_norm)\n",
    "#             # RBF.append(torch.sin((n*torch.pi()/r_cut)*r_ij_norm)/r_ij_norm)\n",
    "#                     # ctx = torch.cat([q, mu_Vn], dim=-1)\n",
    "\n",
    "#             # else:\n",
    "#             #     continue\n",
    "#         f_c=0.5*torch.cos(torch.pi()*r_ij_norm/r_cut)+1\n",
    "#         fcRBF = RBF * f_c\n",
    "#         self.fcRBF=nn.linear(fcRBF,384)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class w(nn.Module):\n",
    "    def __init__(self, r_ij_norm, r_cut):\n",
    "        super().__init__()\n",
    "        N = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "\n",
    "        # Convert r_ij_norm to a numpy array if it's not already\n",
    "        r_ij_norm = np.array(r_ij_norm) if not isinstance(r_ij_norm, np.ndarray) else r_ij_norm\n",
    "\n",
    "        # Calculate RBF values\n",
    "        RBF = [np.sin((n * np.pi / r_cut) * r_ij_norm) / r_ij_norm for n in N]\n",
    "\n",
    "        # Stack RBF values to create a single tensor\n",
    "        RBF = np.stack(RBF, axis=-1)\n",
    "\n",
    "        # Calculate f_c\n",
    "        f_c = 0.5 * np.cos(np.pi * r_ij_norm / r_cut) + 1\n",
    "\n",
    "        # Ensure f_c is expanded to match RBF dimensions for element-wise multiplication\n",
    "        f_c = np.expand_dims(f_c, axis=-1)  # Assuming f_c needs to be expanded\n",
    "\n",
    "        # Element-wise multiplication\n",
    "        fcRBF = RBF * f_c\n",
    "\n",
    "        # Convert fcRBF to a torch tensor\n",
    "        fcRBF = torch.tensor(fcRBF, dtype=torch.float32)\n",
    "\n",
    "        # Define a linear layer\n",
    "        self.fcRBF = nn.Linear(fcRBF.size(-1), 384)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # Assuming x is the input tensor that you want to process\n",
    "    #     # Apply the linear transformation\n",
    "    #     return self.fcRBF(x)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        rbf_out = self.r_RBF(x)\n",
    "        return rbf_out * self.f_c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Message_block(nn.Module):\n",
    "    def __init__(self, phi_input_size, r_ij_norm, r_cut):\n",
    "        super().__init__()\n",
    "        self.phi_layer = phi(phi_input_size)\n",
    "        self.w_layer = w(r_ij_norm, r_cut)\n",
    "\n",
    "        # Maybe fungerer det herover\n",
    "\n",
    "    def forward(self, input1, input2, v_j, s_j, v_norm):\n",
    "        phi_output = self.phi_layer(input1)\n",
    "        w_output = self.w_layer(input2)\n",
    "\n",
    "        ## mÃ¥ske dette i stedet??\n",
    "        # phi = self.net(input1)\n",
    "        # w_output = self.net(input2)\n",
    "\n",
    "        output = phi_output * w_output\n",
    "        output = torch.split(output, 3)\n",
    "        output1 = output[0] * v_j\n",
    "        s_m = torch.sum(output[1]) + s_j\n",
    "        output3 = output[2] * v_norm\n",
    "        v_m = torch.sum(output1 + output3) + v_j\n",
    "        return v_m, s_m\n",
    "\n",
    "\n",
    "### update block\n",
    "\n",
    "class u(nn.Module):\n",
    "    def __init__(self, v_m_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(v_m_size, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class v(nn.Module):\n",
    "    def __init__(self, v_m):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(v_m,128)\n",
    "\n",
    "class s(nn.Module):\n",
    "    def __init__(self, v_norm, s_m):\n",
    "        super().__init__()\n",
    "        activation_fn = nn.SiLU\n",
    "        self.net = nn.Sequential(\n",
    "            # torch.stack(v_norm,s_m),\n",
    "            torch.stack([v_norm, s_m], dim=0), # nyt # vi er nÃ¥et hertil\n",
    "            nn.Linear(256,128),\n",
    "            activation_fn(),\n",
    "            nn.Linear(128,384))\n",
    "\n",
    "class Update_block(nn.Module):\n",
    "    def __init__(self, v_m_size, s_m_size):\n",
    "        super().__init__()\n",
    "        self.u_layer = u(v_m_size)\n",
    "        self.v_layer = v(v_m_size)\n",
    "        self.s_layer = s(v_norm, s_m_size)\n",
    "\n",
    "    def forward(self, input3, input4, input5, v_m, s_m):\n",
    "        U = self.u_layer(input3)\n",
    "        V = self.v_layer(input4)\n",
    "        S = self.s_layer(input5)\n",
    "        V_dup = V.repeat(128, 2)\n",
    "        S = torch.split(S, 3)\n",
    "        output4 = S[0] * U\n",
    "        output5 = S[1] * V_dup\n",
    "        output6 = S[2] + output5\n",
    "        v_u = output4 + v_m\n",
    "        s_u = output6 + s_m\n",
    "        return v_u, s_u\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    def __init__(self, phi_input_size, r_ij, r_cut, v_m_size, s_m_size):\n",
    "        super().__init__()\n",
    "        self.message_block = Message_block(phi_input_size, r_ij, r_cut)\n",
    "        self.update_block = Update_block(v_m_size, s_m_size)\n",
    "\n",
    "\n",
    "    def forward(self, input1, input2, v_j, s_j, v_norm):\n",
    "        # Forward pass through the message block\n",
    "        v_m, s_m = self.message_block(input1, input2, v_j, s_j, v_norm)\n",
    "\n",
    "        # Forward pass through the update block\n",
    "        v_u, s_u = self.update_block(v_m, v_m, s_m, v_m, s_m)\n",
    "\n",
    "        # Return the updated values\n",
    "        return v_u, s_u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_input_size = 128\n",
    "v_m_size = 128\n",
    "s_m_size = 128\n",
    "\n",
    "\n",
    "# input1 = torch.rand(10, phi_input_size)\n",
    "# input2 = torch.rand(10, r_ij.size(1))\n",
    "# v_j = torch.rand(128)\n",
    "# s_j = torch.rand(128)\n",
    "# v_norm = torch.rand(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m PaiNN(phi_input_size, r_ij, r_cut, v_m_size, s_m_size)\n",
      "\u001b[1;32m/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=170'>171</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=171'>172</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmessage_block \u001b[39m=\u001b[39m Message_block(phi_input_size, r_ij, r_cut)\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=172'>173</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_block \u001b[39m=\u001b[39m Update_block(v_m_size, s_m_size)\n",
      "\u001b[1;32m/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu_layer \u001b[39m=\u001b[39m u(v_m_size)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_layer \u001b[39m=\u001b[39m v(v_m_size)\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms_layer \u001b[39m=\u001b[39m s(v_norm, s_m_size)\n",
      "\u001b[1;32m/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=135'>136</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=136'>137</a>\u001b[0m activation_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSiLU\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m     \u001b[39m# torch.stack(v_norm,s_m),\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=139'>140</a>\u001b[0m     torch\u001b[39m.\u001b[39;49mstack([v_norm, s_m], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), \u001b[39m# nyt\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=140'>141</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(\u001b[39m256\u001b[39m,\u001b[39m128\u001b[39m),\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m     activation_fn(),\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Dokumenter/Dokumenter/Git_Projects/02456_DL_Project/notebooks/Network.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(\u001b[39m128\u001b[39m,\u001b[39m384\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got int"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = PaiNN(phi_input_size, r_ij, r_cut, v_m_size, s_m_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input1 = torch.rand(10, phi_input_size)\n",
    "# input2 = torch.rand(10, r_ij.size(1))\n",
    "# v_j = torch.rand(128)\n",
    "# s_j = torch.rand(128)\n",
    "# v_norm = torch.rand(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_u, s_u = model(input1, input2, v_j, s_j, v_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Example loss function, change as needed\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Example optimizer\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(num_epochs):  # num_epochs is the number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input1, input2, v_j, s_j, v_norm)\n",
    "    loss = criterion(outputs, target)  # target needs to be defined\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Message block\n",
    "\n",
    "v_norm = r_ij_tensor/(torch.norm(r_ij_tensor))\n",
    "\n",
    "v_j = torch.zeros(128)\n",
    "\n",
    "class phi(nn.Module):\n",
    "\n",
    "     def __init__(self):\n",
    "        super().__init__()\n",
    "        activation_fn = nn.SiLU\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_j, 128),\n",
    "            activation_fn(),\n",
    "            nn.Linear(128, 384),\n",
    "        )\n",
    "class w(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        RBF = []\n",
    "        for n in range(21):\n",
    "            if i>0:\n",
    "                RBF.append = torch.sin((n*torch.pi()/r_cut)*r_ij)/r_ij\n",
    "            else:\n",
    "                continue\n",
    "        f_c=0.5*torch.cos(torch.pi()*r_ij/r_cut)+1\n",
    "        fcRBF = RBF * f_c\n",
    "        self.fcRBF=nn.linear(fcRBF,384)\n",
    "\n",
    "\n",
    "def forward(self, input1, input2):\n",
    "    phi = self.net(input1)\n",
    "    w = self.net(input2)\n",
    "    output = phi * w\n",
    "    output = torch.split(output,3)\n",
    "    output1 = output[0] * v_j\n",
    "    s_m = torch.sum(output[1]) + s_j\n",
    "    output3 = output[2]*v_norm\n",
    "    v_m= torch.sum(output1+output3) + v_j\n",
    "    return v_m, s_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hansmoeller/Documents/GitHub/02456_DL_Project/notebooks/Network.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hansmoeller/Documents/GitHub/02456_DL_Project/notebooks/Network.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m v_m, s_m \u001b[39m=\u001b[39m forward(\u001b[39mself\u001b[39m, s_j, r_ij)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "v_m, s_m = forward(self, s_j, r_ij) # input1, input2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Update block\n",
    "\n",
    "class u(nn.Module):\n",
    "      def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(v_m,128)\n",
    "class v(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(v_m,128)\n",
    "class s(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        activation_fn = nn.SiLU\n",
    "        self.net = nn.Sequential(\n",
    "            torch.stack(v_norm,s_m),\n",
    "            nn.Linear(256,128),\n",
    "            activation_fn(),\n",
    "            nn.Linear(128,384))\n",
    "\n",
    "def forward(self,input3,input4,input5):\n",
    "    U = self.net(input3)\n",
    "    V = self.net(input4)\n",
    "    S = self.net(input5)\n",
    "    V_dup = V.repeat(128,2)\n",
    "    S = torch.split(S,3)\n",
    "    output4 = S[0]*U\n",
    "    output5 = S[1]*V_dup\n",
    "    output6 = S[2]+output5\n",
    "    v_u = output4 + v_m\n",
    "    s_u = output6 + s_m\n",
    "    return v_u, s_u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Class pain\n",
    "\n",
    "#KÃ¸r 3x message og 3x update (skiftesvis)\n",
    "\n",
    "#Linear, SiLU, Linear, output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class PAINN(nn.Module):\n",
    "#     def __init__(self, n_basis=int(128)):\n",
    "#         super(PAINN, self).__init__()\n",
    "#         self.n_basis = n_basis\n",
    "#         self.embedding = nn.Embedding(16,n_basis,padding_idx=0)\n",
    "\n",
    "#     def x2e(r):\n",
    "#     \"\"\"convert xyz coordinates to pairwise distance\"\"\"\n",
    "#     r2 = np.sqrt(np.sum((r - r[:, np.newaxis, :]) ** 2, axis=-1))\n",
    "#     r_cut = 5\n",
    "#     r_ij = np.where(r2 < r_cut)\n",
    "#     return r_ij\n",
    "\n",
    "\n",
    "\n",
    "#     def init_weights(g, n, m):\n",
    "#     we = np.random.normal(size=(n, m), scale=1e-1)\n",
    "#     wb = np.random.normal(size=(m), scale=1e-1)\n",
    "#     wv = np.random.normal(size=(m, n), scale=1e-1)\n",
    "#     wu = np.random.normal(size=(n, g), scale=1e-1)\n",
    "#     return [we, wb, wv, wu]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class update(nn.Module):\n",
    "\n",
    "class message(nn.Module):\n",
    "\n",
    "     def __init__(self):\n",
    "        super().__init__()\n",
    "        activation_fn = nn.SiLU\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_j, 128),\n",
    "            activation_fn(),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class PAINN(nn.Module):\n",
    "    def __init__(self, n_basis=int(128)):\n",
    "        super(PAINN, self).__init__()\n",
    "        self.n_basis = n_basis\n",
    "        self.embedding = nn.Embedding(16,n_basis,padding_idx=0)\n",
    "\n",
    "    def x2e(r):\n",
    "    \"\"\"convert xyz coordinates to pairwise distance\"\"\"\n",
    "    r2 = np.sqrt(np.sum((r - r[:, np.newaxis, :]) ** 2, axis=-1))\n",
    "    r_cut = 5\n",
    "    r_ij = np.where(r2 < r_cut)\n",
    "    return r_ij\n",
    "\n",
    "    def init_weights(g, n, m):\n",
    "    we = np.random.normal(size=(n, m), scale=1e-1)\n",
    "    wb = np.random.normal(size=(m), scale=1e-1)\n",
    "    wv = np.random.normal(size=(m, n), scale=1e-1)\n",
    "    wu = np.random.normal(size=(n, g), scale=1e-1)\n",
    "    return [we, wb, wv, wu]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'schnetpack'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vonge\\OneDrive\\Dokumenter\\GitHub\\02456_DL_Project\\src\\Network.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/src/Network.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/src/Network.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/src/Network.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mschnetpack\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproperties\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mproperties\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/src/Network.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mschnetpack\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msnn\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vonge/OneDrive/Dokumenter/GitHub/02456_DL_Project/src/Network.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mPaiNN\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPaiNNInteraction\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPaiNNMixing\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'schnetpack'"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import schnetpack.properties as properties\n",
    "import schnetpack.nn as snn\n",
    "\n",
    "__all__ = [\"PaiNN\", \"PaiNNInteraction\", \"PaiNNMixing\"]\n",
    "\n",
    "\n",
    "class PaiNNInteraction(nn.Module):\n",
    "    r\"\"\"PaiNN interaction block for modeling equivariant interactions of atomistic systems.\"\"\"\n",
    "\n",
    "    def __init__(self, n_atom_basis: int, activation: Callable):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_atom_basis: number of features to describe atomic environments.\n",
    "            activation: if None, no activation function is used.\n",
    "            epsilon: stability constant added in norm to prevent numerical instabilities\n",
    "        \"\"\"\n",
    "        super(PaiNNInteraction, self).__init__()\n",
    "        self.n_atom_basis = n_atom_basis\n",
    "\n",
    "        self.interatomic_context_net = nn.Sequential(\n",
    "            snn.Dense(n_atom_basis, n_atom_basis, activation=activation),\n",
    "            snn.Dense(n_atom_basis, 3 * n_atom_basis, activation=None),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        mu: torch.Tensor,\n",
    "        Wij: torch.Tensor,\n",
    "        dir_ij: torch.Tensor,\n",
    "        idx_i: torch.Tensor,\n",
    "        idx_j: torch.Tensor,\n",
    "        n_atoms: int,\n",
    "    ):\n",
    "        \"\"\"Compute interaction output.\n",
    "\n",
    "        Args:\n",
    "            q: scalar input values\n",
    "            mu: vector input values\n",
    "            Wij: filter\n",
    "            idx_i: index of center atom i\n",
    "            idx_j: index of neighbors j\n",
    "\n",
    "        Returns:\n",
    "            atom features after interaction\n",
    "        \"\"\"\n",
    "        # inter-atomic\n",
    "        x = self.interatomic_context_net(q)\n",
    "        xj = x[idx_j]\n",
    "        muj = mu[idx_j]\n",
    "        x = Wij * xj\n",
    "\n",
    "        dq, dmuR, dmumu = torch.split(x, self.n_atom_basis, dim=-1)\n",
    "        dq = snn.scatter_add(dq, idx_i, dim_size=n_atoms)\n",
    "        dmu = dmuR * dir_ij[..., None] + dmumu * muj\n",
    "        dmu = snn.scatter_add(dmu, idx_i, dim_size=n_atoms)\n",
    "\n",
    "        q = q + dq\n",
    "        mu = mu + dmu\n",
    "\n",
    "        return q, mu\n",
    "\n",
    "\n",
    "class PaiNNMixing(nn.Module):\n",
    "    r\"\"\"PaiNN interaction block for mixing on atom features.\"\"\"\n",
    "\n",
    "    def __init__(self, n_atom_basis: int, activation: Callable, epsilon: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_atom_basis: number of features to describe atomic environments.\n",
    "            activation: if None, no activation function is used.\n",
    "            epsilon: stability constant added in norm to prevent numerical instabilities\n",
    "        \"\"\"\n",
    "        super(PaiNNMixing, self).__init__()\n",
    "        self.n_atom_basis = n_atom_basis\n",
    "\n",
    "        self.intraatomic_context_net = nn.Sequential(\n",
    "            snn.Dense(2 * n_atom_basis, n_atom_basis, activation=activation),\n",
    "            snn.Dense(n_atom_basis, 3 * n_atom_basis, activation=None),\n",
    "        )\n",
    "        self.mu_channel_mix = snn.Dense(\n",
    "            n_atom_basis, 2 * n_atom_basis, activation=None, bias=False\n",
    "        )\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, q: torch.Tensor, mu: torch.Tensor):\n",
    "        \"\"\"Compute intraatomic mixing.\n",
    "\n",
    "        Args:\n",
    "            q: scalar input values\n",
    "            mu: vector input values\n",
    "\n",
    "        Returns:\n",
    "            atom features after interaction\n",
    "        \"\"\"\n",
    "        ## intra-atomic\n",
    "        mu_mix = self.mu_channel_mix(mu)\n",
    "        mu_V, mu_W = torch.split(mu_mix, self.n_atom_basis, dim=-1)\n",
    "        mu_Vn = torch.sqrt(torch.sum(mu_V**2, dim=-2, keepdim=True) + self.epsilon)\n",
    "\n",
    "        ctx = torch.cat([q, mu_Vn], dim=-1)\n",
    "        x = self.intraatomic_context_net(ctx)\n",
    "\n",
    "        dq_intra, dmu_intra, dqmu_intra = torch.split(x, self.n_atom_basis, dim=-1)\n",
    "        dmu_intra = dmu_intra * mu_W\n",
    "\n",
    "        dqmu_intra = dqmu_intra * torch.sum(mu_V * mu_W, dim=1, keepdim=True)\n",
    "\n",
    "        q = q + dq_intra + dqmu_intra\n",
    "        mu = mu + dmu_intra\n",
    "        return q, mu\n",
    "\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    \"\"\"PaiNN - polarizable interaction neural network\n",
    "\n",
    "    References:\n",
    "\n",
    "    .. [#painn1] SchÃ¼tt, Unke, Gastegger:\n",
    "       Equivariant message passing for the prediction of tensorial properties and molecular spectra.\n",
    "       ICML 2021, http://proceedings.mlr.press/v139/schutt21a.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_atom_basis: int,\n",
    "        n_interactions: int,\n",
    "        radial_basis: nn.Module,\n",
    "        cutoff_fn: Optional[Callable] = None,\n",
    "        activation: Optional[Callable] = F.silu,\n",
    "        max_z: int = 100,\n",
    "        shared_interactions: bool = False,\n",
    "        shared_filters: bool = False,\n",
    "        epsilon: float = 1e-8,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_atom_basis: number of features to describe atomic environments.\n",
    "                This determines the size of each embedding vector; i.e. embeddings_dim.\n",
    "            n_interactions: number of interaction blocks.\n",
    "            radial_basis: layer for expanding interatomic distances in a basis set\n",
    "            cutoff_fn: cutoff function\n",
    "            activation: activation function\n",
    "            shared_interactions: if True, share the weights across\n",
    "                interaction blocks.\n",
    "            shared_interactions: if True, share the weights across\n",
    "                filter-generating networks.\n",
    "            epsilon: stability constant added in norm to prevent numerical instabilities\n",
    "        \"\"\"\n",
    "        super(PaiNN, self).__init__()\n",
    "\n",
    "        self.n_atom_basis = n_atom_basis\n",
    "        self.n_interactions = n_interactions\n",
    "        self.cutoff_fn = cutoff_fn\n",
    "        self.cutoff = cutoff_fn.cutoff\n",
    "        self.radial_basis = radial_basis\n",
    "\n",
    "        self.embedding = nn.Embedding(max_z, n_atom_basis, padding_idx=0)\n",
    "\n",
    "        self.share_filters = shared_filters\n",
    "\n",
    "        if shared_filters:\n",
    "            self.filter_net = snn.Dense(\n",
    "                self.radial_basis.n_rbf, 3 * n_atom_basis, activation=None\n",
    "            )\n",
    "        else:\n",
    "            self.filter_net = snn.Dense(\n",
    "                self.radial_basis.n_rbf,\n",
    "                self.n_interactions * n_atom_basis * 3,\n",
    "                activation=None,\n",
    "            )\n",
    "\n",
    "        self.interactions = snn.replicate_module(\n",
    "            lambda: PaiNNInteraction(\n",
    "                n_atom_basis=self.n_atom_basis, activation=activation\n",
    "            ),\n",
    "            self.n_interactions,\n",
    "            shared_interactions,\n",
    "        )\n",
    "        self.mixing = snn.replicate_module(\n",
    "            lambda: PaiNNMixing(\n",
    "                n_atom_basis=self.n_atom_basis, activation=activation, epsilon=epsilon\n",
    "            ),\n",
    "            self.n_interactions,\n",
    "            shared_interactions,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Compute atomic representations/embeddings.\n",
    "\n",
    "        Args:\n",
    "            inputs (dict of torch.Tensor): SchNetPack dictionary of input tensors.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: atom-wise representation.\n",
    "            list of torch.Tensor: intermediate atom-wise representations, if\n",
    "            return_intermediate=True was used.\n",
    "        \"\"\"\n",
    "        # get tensors from input dictionary\n",
    "        atomic_numbers = inputs[properties.Z]\n",
    "        r_ij = inputs[properties.Rij]\n",
    "        idx_i = inputs[properties.idx_i]\n",
    "        idx_j = inputs[properties.idx_j]\n",
    "        n_atoms = atomic_numbers.shape[0]\n",
    "\n",
    "        # compute atom and pair features\n",
    "        d_ij = torch.norm(r_ij, dim=1, keepdim=True)\n",
    "        dir_ij = r_ij / d_ij\n",
    "        phi_ij = self.radial_basis(d_ij)\n",
    "        fcut = self.cutoff_fn(d_ij)\n",
    "\n",
    "        filters = self.filter_net(phi_ij) * fcut[..., None]\n",
    "        if self.share_filters:\n",
    "            filter_list = [filters] * self.n_interactions\n",
    "        else:\n",
    "            filter_list = torch.split(filters, 3 * self.n_atom_basis, dim=-1)\n",
    "\n",
    "        q = self.embedding(atomic_numbers)[:, None]\n",
    "        qs = q.shape\n",
    "        mu = torch.zeros((qs[0], 3, qs[2]), device=q.device)\n",
    "\n",
    "        for i, (interaction, mixing) in enumerate(zip(self.interactions, self.mixing)):\n",
    "            q, mu = interaction(q, mu, filter_list[i], dir_ij, idx_i, idx_j, n_atoms)\n",
    "            q, mu = mixing(q, mu)\n",
    "\n",
    "        q = q.squeeze(1)\n",
    "\n",
    "        inputs[\"scalar_representation\"] = q\n",
    "        inputs[\"vector_representation\"] = mu\n",
    "        return inputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
